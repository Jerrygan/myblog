## redis偶尔连接超时，实际生产的bigKey

查找发现：联系了运维组同事拿到了rdb文件，利用了github上面的go语言写的一个工具，执行分析出来了

实际效果比redis自带的好，自带的只显示每种数据结构超过的bigkey，最大那个，并不是所有

解决：设定阈值，key-field-value结构，key-活动-手机号码，对value取模或者hash，然后将key+值作为新key分键保存，就是分治的思想

## 缓存hotkey

https://mp.weixin.qq.com/s/EoCu3IZcbXeYmVcfLDs2Jw

### 发现热key

*方法一:凭借业务经验，进行预估哪些是热key*
其实这个方法还是挺有可行性的。比如某商品在做秒杀，那这个商品的key就可以判断出是热key。缺点很明显，并非所有业务都能预估出哪些key是热key。
*方法二:在客户端进行收集*
这个方式就是在操作redis之前，加入一行代码进行数据统计。那么这个数据统计的方式有很多种，也可以是给外部的通讯系统发送一个通知信息。缺点就是对客户端代码造成入侵。
*方法三:在Proxy层做收集*
有些集群架构是下面这样的，Proxy可以是Twemproxy，是统一的入口。可以在Proxy层做收集上报，但是缺点很明显，并非所有的redis集群架构都有proxy

*方法四:用redis自带命令*

### 如何解决

目前业内的方案有两种
*(1)利用二级缓存*
比如利用`ehcache`，或者一个`HashMap`都可以。在你发现热key以后，把热key加载到系统的JVM中。
针对这种热key请求，会直接从jvm中取，而不会走到redis层。
假设此时有十万个针对同一个key的请求过来,如果没有本地缓存，这十万个请求就直接怼到同一台redis上了。
现在假设，你的应用层有50台机器，OK，你也有jvm缓存了。这十万个请求平均分散开来，每个机器有2000个请求，会从JVM中取到value值，然后返回数据。避免了十万个请求怼到同一台redis上的情形。
*(2)备份热key*
这个方案也很简单。不要让key走到同一台redis上不就行了。我们把这个key，在多个redis上都存一份不就好了。接下来，有热key请求进来的时候，我们就在有备份的redis上随机选取一台，进行访问取值，返回数据。
假设redis的集群数量为N，步骤如下图所示

![image-20210524011320468](G:\dev\typora-image\image-20210524011320468.png)

```
const M = N * 2
//生成随机数
random = GenRandom(0, M)
//构造备份新key
bakHotKey = hotKey + “_” + random
data = redis.GET(bakHotKey)
if data == NULL {
    data = GetFromDB()
    redis.SET(bakHotKey, expireTime + GenRandom(0,5))
}
```

问题：如何自动监控hotKey

*(2)通知系统做处理*
在这个角度，有赞用的是上面的解决方案一:利用二级缓存进行处理。
有赞在监控到热key后，Hermes服务端集群会通过各种手段通知各业务系统里的Hermes-SDK，告诉他们:"老弟，这个key是热key，记得做本地缓存。"
于是Hermes-SDK就会将该key缓存在本地，对于后面的请求。Hermes-SDK发现这个是一个热key，直接从本地中拿，而不会去访问集群。

除了这种通知方式以外。我们也可以这么做，比如你的流式计算系统监控到热key了，往zookeeper里头的某个节点里写。然后你的业务系统监听该节点，发现节点数据变化了，就代表发现热key。最后往本地缓存里写，也是可以的。

## 阿里解方案

![image-20210524011648644](G:\dev\typora-image\image-20210524011648644.png)

架构中各节点的作用如下：

- SLB 层做负载均衡
- Proxy 层做读写分离自动路由
- Master 负责写请求
- ReadOnly 节点负责读请求
- Slave 节点和 Master 节点做高可用

实际过程中 Client 将请求传到 SLB，SLB 又将其分发至多个 Proxy 内，通过 Proxy 对请求的识别，将其进行分类发送。例如，将同为 Write 的请求发送到 Master 模块内，而将 Read 的请求发送至 ReadOnly 模块。而模块中的只读节点可以进一步扩充，从而有效解决热点读的问题。读写分离同时具有可以灵活扩容读热点能力、可以存储大量热点Key、对客户端友好等优点。

![image-20210524011827012](G:\dev\typora-image\image-20210524011827012.png)

该方案通过主动发现热点并对其进行存储来解决热点 Key 的问题。首先 Client 也会访问 SLB，并且通过 SLB 将各种请求分发至 Proxy 中，Proxy 会按照基于路由的方式将请求转发至后端的 Redis 中。

在热点 key 的解决上是采用在服务端增加缓存的方式进行。具体来说就是在 Proxy 上增加本地缓存，本地缓存采用 LRU 算法来缓存热点数据，后端 db 节点增加热点数据计算模块来返回热点数据。

Proxy 架构的主要有以下优点：

- Proxy 本地缓存热点，读能力可水平扩展
- DB 节点定时计算热点数据集合
- DB 反馈 Proxy 热点数据
- 对客户端完全透明，不需做任何兼容

![image-20210524012108375](G:\dev\typora-image\image-20210524012108375.png)

在热点 Key 的处理上主要分为写入跟读取两种形式，在数据写入过程当 SLB 收到数据 K1 并将其通过某一个 Proxy 写入一个 Redis，完成数据的写入。假若经过后端热点模块计算发现 K1 成为热点 key 后， Proxy 会将该热点进行缓存，当下次客户端再进行访问 K1 时，可以不经 Redis。最后由于 proxy 是可以水平扩充的，因此可以任意增强热点数据的访问能力。

![image-20210524012127451](G:\dev\typora-image\image-20210524012127451.png)

对于 db 上热点数据的发现，首先会在一个周期内对 Key 进行请求统计，在达到请求量级后会对热点 Key 进行热点定位，并将所有的热点 Key 放入一个小的 LRU 链表内，在通过 Proxy 请求进行访问时，若 Redis 发现待访点是一个热点，就会进入一个反馈阶段，同时对该数据进行标记。

DB 计算热点时，主要运用的方法和优势有：

- 基于统计阀值的热点统计
- 基于统计周期的热点统计
- 基于版本号实现的无需重置初值统计方法
- DB 计算同时具有对性能影响极其微小、内存占用极其微小等优点

## JVM线上调优

https://mp.weixin.qq.com/s/i_1mcepefra6TzvVn19O2w

#### 实现了：通过这一个多月的努力，将FullGC从40次/天优化到近10天才触发一次，而且YoungGC的时间也减少了一半以上，这么大的优化，

#### 原来是：前一段时间，线上服务器的FullGC非常频繁，平均一天40多次

首先线上部署服务资源

4台服务器集群很一般的2核4G，

当时大概参数好像是初始化堆内存在1000M（-Xms -Xmx），设置的年轻代大小就开始就很小，好像是300多M，新生代跟老年代比例是默认的3/8，还有一个堆栈大小也是关注的300K左右

```
-Xms1000M -Xmx1800M -Xmn350M -Xss300K -XX:+DisableExplicitGC -XX:SurvivorRatio=4 -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled -XX:LargePageSizeInBytes=128M -XX:+UseFastAccessorMethods -XX:+UseCMSInitiatingOccupancyOnly -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC
```

- -Xmx1800M：设置JVM最大可用内存为1800M。
- -Xms1000m：设置JVM初始化内存为1000m。此值可以设置与-Xmx相同，以避免每次垃圾回收完成后JVM重新分配内存。
- -Xmn350M：设置年轻代大小为350M。整个JVM内存大小=年轻代大小 + 年老代大小 + 持久代大小。持久代一般固定大小为64m，所以增大年轻代后，将会减小年老代大小。此值对系统性能影响较大，Sun官方推荐配置为整个堆的3/8。
- -Xss300K：设置每个线程的堆栈大小。JDK5.0以后每个线程堆栈大小为1M，以前每个线程堆栈大小为256K。更具应用的线程所需内存大小进行调整。在相同物理内存下，减小这个值能生成更多的线程。但是操作系统对一个进程内的线程数还是有限制的，不能无限生成，经验值在3000~5000左右。

### 第一次优化

- 调整新生代大小，新生代收集就耗时800多s

-Xmn350M -> -Xmn800M
-XX:SurvivorRatio=4 -> -XX:SurvivorRatio=8
-Xms1000m ->-Xms1800m

拿了两台做启动优化

运行了5天后，观察GC结果，YoungGC减少了一半以上的次数，时间减少了400s，但是FullGC的平均次数增加了41次。YoungGC基本符合预期设想，但是这个FullGC就完全不行了

### 第二次优化

发现了内存泄漏的问题

有个对象在内存中有一万多个实例，而且这些实例占据了将近20M的内存，用到了threadLocal，对订单的数据做了本地维护在内存了，了解到threadLocal的是弱引用，但是value如果不手动调remove()是一直不会被回收

发现前三次GullGC时，老年代占据的内存还不足30%，却发生了FullGC

metaspace导致FullGC的情况，服务器默认的metaspace是21M=》排查log发现最大都到达了200M

## 总结

通过这一个多月的调优总结出以下几点：

- FullGC一天超过一次肯定就不正常了
- 发现FullGC频繁的时候优先调查内存泄漏问题
- 内存泄漏解决后，jvm可以调优的空间就比较少了，作为学习还可以，否则不要投入太多的时间
- 如果发现CPU持续偏高，排除代码问题后可以找运维咨询下阿里云客服，这次调查过程中就发现CPU 100%是由于服务器问题导致的，进行服务器迁移后就正常了。
- 数据查询的时候也是算作服务器的入口流量的，如果访问业务没有这么大量，而且没有攻击的问题的话可以往数据库方面调查
- 有必要时常关注服务器的GC，可以及早发现问题